Kafka Deep Dive :-

Usage : Used mainly for high throughput event streaming and message queing.

Components

1. Producers (Publish messages using Producer Api)
2. Consumers (Consume messages using Consumer Api)
3. Kafka Cluster (Holds the brain of the system ,eg - All consumers commit their offsetr values to the cluster for use in case they go down)
4. Message Brokers (Receives messages and routes them to the designated partition)
5. Partition (Used for separating traffic by use case of scalability)
6. Topics (Logical division of messages in the application itself)
7. Partition Key (Present in every message. Used to decide which partition , this message is destined for.)
8. Consumer Groups (Used primarily to ensure that a message from a topic is consumed only once. Genrally every consumer group has a designated topic. Every consumer in a group has a designated partition range from which to consume.)

Deep Dives :- 

1. Scalability
    - Create multiple brokers (1 broker typically can be assumed to handle 10k messages and total 1 TB data offcourse with suitable hardware)
    - Aim to keep message size < 1 MB
    - Choose a good partitioning key (so that messages are evenly distributed across partitions)
    - How to deal with a hot partition ? - Add a suffix to the key,eg If the original key was addidas-key, make it as addidas-key:n where n is (1,10). So this way , the traffic gets distributed within 10 ndifferent partitions. 
2. Duarability and Fault Tolerance
    - Each partition has replicas (called followers). The original one is called the leader and it has followers who sync up the messages from the leader and stay prepared to take over incase the leader goes down.
    - There are 2 main configs
        replicas - no.of followers 
        ack - the no.of followers that need to acknowledge a message before moving on the next one
        Higher the ack - more is the durability but slower are the operations and vice versa. 
3. Error and Retries
     
4. Data Resilience
5. Performance Optimization

